---
title: "Metrics of disconcordance with `fasthplus`"
author: "Nathan Dyjack and Stephanie Hicks"
date: "Last modified: October 18, 2021; Compiled: `r format(Sys.time(), '%B %d, %Y')`"
bibliography: biblio.bib
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{fasthplus vignette}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

### Installation

At present, `fasthplus` is only available for installation via github using the `devtools` package.
A `CRAN` distribution is in preparation and this section will be updated to reflect its eventual availability.

```{r, eval=FALSE, echo=FALSE}
if(!requireNamespace('devtools')){
  install.packages('devtools')
}
```

```{r, eval=FALSE}
devtools::install_github(repo="ntdyjack/fasthplus", 
                         ref = "main", 
                         build_vignettes = TRUE, 
                         subdir = NULL, 
                         auth_token = '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')
```

Once the package is installed, you can load the library

```{r}
library(fasthplus)
```

This vignette serves as an introductory example on how to utilize the `fasthplus` package.


# Introduction

A standard analysis using high-throughput sequencing data is to cluster observations using a dissimilarity measure. For example, clustering methods that use Euclidean distance are often used with normalized and log2-transformed data. If there does not exist a gold-standard or ground-truth label for each observation, performance measures based on the tightness or consistency of the cluster are often used. 

However, when assessing the performance using different dissimilarity measures, the interpretation of these metrics can be problematic as different dissimilarity measures have different magnitudes and ranges of values that they span. To address this problem, here we propose a new performance metric called $H_{+}$, which is an extension of the 'scale-agnostic' $G_{+}$ disconcordance metric. 

## disconcordance 

Generally, disconcordance can be described using two sets $A$ and $B$ by comparing their elements

$$ s=\sum_{a\in A}\sum_{b\in B}1_{[a>b]} $$

Also, we can consider a special case that explores the clustering problem for a given data set with $n$ observations. 

In the context of the clustering (or identifying predicted class labels), **disconcordance** can be conceptualized as measure that assess how well a given predicted class (or cluster) label fits the observed data.
A typical clustering algorithm utilizes a dissimilarity matrix $D(n\times n)$, with $N_{d}=n(n-1)/2$ unique pairs which would correspond to the upper diagonal entries of $D$.

$$
D=\begin{bmatrix}\begin{array}{cccc}
d_{11} & d_{12} & \cdots & d_{1n}\\
 & d_{22} & \cdots & d_{2n}\\
 &  & \ddots & \vdots\\
 &  &  & d_{nn}
\end{array}\end{bmatrix}
$$

When clustering $n$ observations, one can fix $D$ and place each of $n$ cells in one of $k$ clusters by optimizing some fitness criteria. Cluster fitness relates to how well a set of predicted cluster labels $L=[l_{1},\dots,l_{n}:l_{i}=1\dots,k]$ explain the observed data, for example, using the sum of within-cluster dissimilarities (dispersion).

In this case, for fixed $L$, we can generate an adjacency matrix that tells us whether each observation has the same label (i.e., falls in the same cluster)

$$
A=\begin{bmatrix}\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1n}\\
 & a_{22} & \cdots & a_{2n}\\
 &  & \ddots & \vdots\\
 &  &  & a_{nn}
\end{array}\end{bmatrix}\hspace{1em}a_{ij}=\begin{array}{cc}
1 & \mathrm{if}\enspace l_{i}=l_{j}\\
0 & \mathrm{otherwise}
\end{array}
$$

In terms of our notation, $s$ can be written as

$$
s=\sum_{i=2}^{n}\sum_{j<i}1(a_{ij}=1)\sum_{k=2}^{n}\sum_{l<k}1(a_{kl}=0)1(d_{ij}>d_{kl})
$$

## $G_{+}$ disconcordance metric

Previous work includes the $G_{+}$ disconcordance metric [@williams1971comparison] [@rohlf1974methods] currently available in `R` in the `clusterCrit` package [@desgraupes2018package], where $s$ is scaled by $N_{d}(N_{d}-1)/2={N_{d} \choose 2}$, the total number of ways to compare each unique distance to every other.

$$
G_{+}=\frac{s}{N_{d}(N_{d}-1)/2}
$$

If $D$ is fixed, lower values of $G_{+}$ among many sets of labels $L_{1},L_{2}\dots$ indicate increased label fitness. If we instead fix $L$, we can also use disconcordance to assess the fitness of multiple dissimilarity matrices $D_{1},D_{2}\dots$ [@rohlf1974methods].

Since each discordance value in $G_{+}$ is derived from the relative rankings of distances, this transformation allows us to compare distances by the structure they impose on the data rather than by the exact values of the distances.
This allows us to evaluate distances at highly varying scales without imposing bias with regard to the expected magnitude of distances.
In other words, disconcordance metrics such as $G_{+}$ offers a 'scale-agnostic' means to asses cluster fitness.

## $H_{+}$ disconcordance metric

However, one problem with $G_{+}$ is that it is biased (Dyjack et al. 2021). Specifically, as $\frac{N_{d}}{N_{d}-1} \to 1$, $G_{+}$ is a biased by a factor of $2\alpha(1-\alpha)$ where $\alpha$ is the proportion of $N_d$ distances that are within-cluster. 

```{r motivating-figure, echo=FALSE, fig.cap='[$G_{+}$ is biased and a function of the proportion of within-cluster distances while $H_{+}$ is unbiased]', out.width = '80%', fig.align='center'}
knitr::include_graphics("motivating_figure.jpeg")
```


Hence, if we parameterize the portion of $N_{d}$ that are $d_{ij}\in D_{w}$ such that $\lvert D_{W} \rvert = \alpha N_{d}$, $\alpha \in(0,1)$, and similarly, $\lvert D_{B} \rvert = (1-\alpha) N_{d}$ (and $\lvert D_{W}\rvert+\lvert D_{B}\rvert=N_{d}$), then we can replace the denominator in $G_{+}$ ($N_{d}(N_{d}-1)/2$) with $\lvert D_{W}\rvert \lvert D_{B}\rvert$ in our new disconcordance metric: 

$$ 
H_{+}=\frac{s}{\lvert D_{W}\rvert \lvert D_{B}\rvert}
$$

In other words, instead of scaling $s$ by the total number of ways to compare every distance to every other distance, we divide by the number of ways to compare within-cluster distances to between-cluster distances, which we show is no longer biased by $\alpha$ [Dyjack et al. (2021)]

### Interpreting $H_{+}$ with two sets $A$ and $B$

$H_{+}$ can be thought of as the product of two parameters $\gamma_{A},\gamma_{B}$, which lends itself to a simple interpretation for $H_{+}$: $\gamma_{A}\times100\%$ of $a \in A$ are strictly greater than $\gamma_{B}\times100\%$ of $b \in B$.

We provide two equivalent algorithms for this estimation process, with the further benefit that our algorithms yields a range of reasonable values for $\gamma_{A},\gamma_{B}$.

For further exploration of disconcordance, these estimators ($G_{+}$ and $H_{+}$), as well as their theoretical properties, please see Dyjack et al. (2021).


# Using `hpe()` in `fasthplus`

The `fasthplus` package includes two fast algorithms to estimate $H_{+}$. 

The primary workhorse function in `fasthplus` is the `hpe()` (H Plus Estimator) function. As input, `hpe()` can estimate $H_{+}$ using **either**:

1. Two arbitrary vectors of length $n$: $A = \{a_1, \ldots, a_n\}$ and $B = \{b_1, \ldots, b_n\}$
2. A dissimilarity matrix $D$ of dimension $n \times n$ and set of labels $L$  of length $n$

The `hpe()` returns a numeric value for $H_{+}$ given $A,B$ or $D,L$.

In comparison to similar cluster-fitness packages (`clusterCrit`), which induce Euclidean distance from the observations, `fasthplus` is designed to handle an arbitrary dissimilarity matrix.

## Additional arguments

In addition to either $A,B$ or $D,L$, the user may also provide `hpe()` with two additional arguments: `p` and `alg`

### `p`

An `integer` value that specifies the number of percentiles to calculate for each set $A,B$.
`p` is used to specify the desired accuracy of the $H_{+}$ estimate, where the theoretical accuracy is guaranteed within $\pm \frac{1}{p-1}$.
The default value `p=101`, that is, an accuracy of 0.01. 

### `alg`

A character string referring to two algorithms used to estimate $H_{+}$.
For most values of $p$ the algorithms have comparable performance.

- `alg = "brute_force"`: (default) this is a 'brute-force' estimation that performs best (in `R`) for smaller values of $p$.
- `alg = "grid_search"`is a percentile-based ($p$), grid search solution for the same value that dramatically reduces the number of comparisons compared to the number of calculations than Algorithm 1. In Dyjack et al. (2021), we derive a numerical bound for the accuracy of this estimator as a function of $p$, the number of percentiles taken from the two sets of interest. The user can specify $p$, and `hpe()` guarantees accuracy within $\pm \frac{1}{p-1}$ of the true $H_{+}$.

In practice, the two algorithms have similar performance for most values of $p$. However, as Algorithm 2 performs strictly less calculations than Algorithm 1, we suggest Algorithm 2 for any $p > 101$.

### `alphas`

A boolean value which indicates whether `hpe()`  should include the values for $\alpha_{W}$ and $\alpha_{B}$ , the portion of dissimilarities that are either between within and between-cluster.
These $\alpha$ values can be used to form a hueristic for comparing two labels $L_{1},L_{2} which have different balances of between and within-cluster dissimilarities.
To compare $H_{+,1},H_{+,2}$ induced from labels $L_{1},L_{2}$, we suggest scaling one label to the $\alpha$ balance of the other: $H`_{+,2}= H_{+,2} \cdot \frac{\alpha_{2}(1-\alpha{2})}{\alpha_{1}(1-\alpha{1})}$.
The lesser of $H_{+,1}$ and $H_{+,2}'$  then indicates the less disconcordant of the two labels.

# Examples

To demonstrate `hpe()`, we consider several simulated data sets:

## Example 1: $A,B$ formulation

This formulation seeks to quantify the answer a simple question: for two sets $A,B$ how often can we expect that $a>b,a \in A,b \in B$?

We simulate $A$ and $B$ as $n=10000$ draws from a univariate normal distributions with unit variance and slightly different means ($\mu_{A}=0.5,\mu_{B}=-0.5$). 

```{r, fig.width=6,fig.height=4}
set.seed(1234)
n <- 10000
a <- rnorm(n=n,mean=0.5,sd=1)
b <- rnorm(n=n,mean=-0.5,sd=1)

bins <- seq(min(c(a,b)),max(c(a,b)),length.out=20)
hist(x=a,breaks=bins,main='',xlab='',ylab='Frequency',plot=T, border='blue',col='#0000ff64', freq=T)
hist(x=b,breaks=bins,add=T,border='red',col='#ff000064',freq=T)
legend('topright',legend=c("A","B"), pch=c(22,22),
  col= c('blue','red'),cex=1.5, pt.bg=c('#0000ff64','#ff000064'),bty='n')
```

Here is our estimate of $H_{+}$: 

```{r}
hpe(A=a,B=b) #A, B formulation
```


## Example 2: $D,L$ formulation

We can apply $A,B$ formulation to a dissimilarity matrix $D$ and set of cluster labels $L$.
$L$ can be used to generate a binary adjacency matrix that tells us (for every unique pair of observations) whether two observations belong to the same group.
This adjacency matrix (more specifically, its upper-triangular elements) can then be used to classify every unique dissimilarity $d\in D$ as corresponding to a pair within the same cluster or not.

We can now define $A$ as the unique within-cluster distnaces ($D_{W}$), and $B$ as the unique between-cluster distances ($D_{B}$), and the problem as been reduced to the $A,B$ formulation.

We simulate two sets of $n=1000$ observations (Cluster 1 and 2) with each observation corresponding to multiple ($m=100$) draws from the same distributions describe in the previous sections.

```{r,fig.width=6,fig.height=4}
n <- 1000
m <- 100
cl1 <- sapply(1:n, function(i) rnorm(n=m,mean=0.5,sd=1))
cl2 <- sapply(1:n, function(i) rnorm(n=m,mean=-0.5,sd=1))
dat <- cbind(cl1,cl2)
d <- dist(t(dat))
dvec <- as.matrix(d)
dvec <- dvec[upper.tri(dvec)]
l <- c(rep(0,n),rep(1,n))
ind <- sapply(l, function(x) x==l)
ind <- ind[upper.tri(ind)]
iw <- which(ind)
ib <- which(!ind)
dw <- dvec[iw]
db <- dvec[ib]

bins <- seq(min(dvec),max(dvec),length.out=20)
hist(x=dw,breaks=bins,main='',xlab='',ylab='Frequency',plot=T, border='blue',col='#0000ff64', freq=T)
hist(x=db,breaks=bins,add=T,border='red',col='#ff000064',freq=T)
legend('topright',legend=c(expression('D'[W]), expression('D'[B])), pch=c(22,22),
  col= c('blue','red'),cex=1.5, pt.bg=c('#0000ff64','#ff000064'),bty='n')
```

Here is our estimate of $H_{+}$: 

```{r}
hpe(D=d,L=l) # D, L formulation
```

For comparison, we also demonstrate calculation of $G_{+}$ using the `clusterCrit` package.
Note that `clusterCrit` takes the data `dat` directly as an argument, while `fasthplus` takes an arbitrary dissimilarity matrix induced from the same data.

```{r}
library(clusterCrit)
intCriteria(traj=t(dat),part=as.integer(l),crit='G_plus')
```



## Example 3: developing intuition for true value of $H_{+}$

Here we demonstrate the behaviors of $H_{+}$ and `hpe()` for two additional simulated datasets in the $D,L$ formulation.

We hope that this section provides some intuition for the true value of $H_{+}$ in these scenarios, and how this value compares to the estimate given by `hpe()`.

### Example 3.1: Two highly similar clusters

We simulate $n=1000$ observations from the same distribution.
Each observation is randomly (with probability $0.5$) membership in one of two clusters.
In this way, we explore how $H_{+}$ ang $G_{+}$ behave in the case where a cluster set $L$ has not provided any more information than we might attain by chance.

```{r,fig.width=6,fig.height=4}
n <- 1000
m <- 100
dat <- sapply(1:n, function(i) rnorm(n=m,mean=0,sd=1))
d <- dist(t(dat))
pc <- prcomp(t(dat))$x[,1:2]
l <- round(runif(n=n))
cols <- ifelse(l==1,'#0000ff64','#ff000064')
plot(x=pc[,1],y=pc[,2],pch=16,col=cols,cex=0.7,xaxs = "i",yaxs = "i",xlab='PC1',ylab='PC2',xaxt='n',yaxt='n')
legend('topleft',legend=c(expression('Cl'[1]), expression('Cl'[2])), pch=c(21,21),
  col= c('blue','red'),cex=1.5, pt.bg=c('#0000ff64','#ff000064'),bty='n')
```

```{r}
hpe(D=d,L=l) # D,L formulation
intCriteria(traj=t(dat),part=as.integer(l),crit='G_plus')
```


### Example 3.2: Two distinct clusters

We simulate $n=1000$ observations from the two distinct distributions.
Each observation is assigned a cluster label corresponding to the distribution from which it was drawn.
In this way, we explore how $H_{+}$ and $G_{+}$ behave in the case where a cluster set $L$ gives more information that we might attain by chance

```{r,fig.width=6,fig.height=4}
n <- 500
cl1 <- sapply(1:n, function(i) rnorm(n=m,mean=0.5,sd=1))
cl2 <- sapply(1:n, function(i) rnorm(n=m,mean=-0.5,sd=1))
dat <- cbind(cl1,cl2)
d <- dist(t(dat))
l <- c(rep(0,n),rep(1,n))
pc <- prcomp(t(dat))$x[,1:2]
cols <- ifelse(l==1,'#0000ff64','#ff000064')
plot(x=pc[,1],y=pc[,2],pch=16,col=cols,cex=0.7,xaxs = "i",yaxs = "i",xlab='PC1',ylab='PC2',xaxt='n',yaxt='n')
legend('top',legend=c(expression('Cl'[1]), expression('Cl'[2])), pch=c(21,21),
  col= c('blue','red'),cex=1.5, pt.bg=c('#0000ff64','#ff000064'),bty='n')
```

```{r}
hpe(D=d,L=l) #D,L formulation
intCriteria(traj=t(dat),part=as.integer(l),crit='G_plus')
```

# References
